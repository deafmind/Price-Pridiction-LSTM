{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f038d8",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Stock Price Prediction\n",
    "### This NoteBook loads stock price data, performs cleaning, feature engineering, scaling, and sequence creation for LSTM model training.\n",
    "### The processed data will be saved for use in the LSTM training ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c329e1",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc56dd",
   "metadata": {},
   "source": [
    "### Load the stock price data from a CSV file and perform initial inspection and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e741ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/Google.csv\")\n",
    "df = pd.DataFrame(data=data)\n",
    "print(df.head())\n",
    "\n",
    "df.rename(columns={'Unnamed: 0': 'timestamp'}, inplace=True)\n",
    "print(df.isnull().sum())\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(df.info())\n",
    "df.set_index('timestamp', inplace=True)\n",
    "if df.index.duplicated().any():\n",
    "    print(f\"\\nFound {df.index.duplicated().sum()} duplicate timestamps. Removing duplicates...\")\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    print(\"Duplicates removed. DataFrame head:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nNo duplicate timestamps found.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc13f6",
   "metadata": {},
   "source": [
    "## 2. Create Dummy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf185ea",
   "metadata": {},
   "source": [
    "### For testing purposes, create a dummy dataset to ensure reproducibility. In a real scenario, use the actual data loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "date_range = pd.date_range(start='2016-01-01', periods=200, freq='B')  # 200 business days\n",
    "np.random.seed(42)\n",
    "dummy_data = {\n",
    "    'open': np.random.uniform(30, 50, 200).cumsum() + 100,\n",
    "    'high': np.random.uniform(30, 50, 200).cumsum() + 101,\n",
    "    'low': np.random.uniform(30, 50, 200).cumsum() + 99,\n",
    "    'close': np.random.uniform(30, 50, 200).cumsum() + 100,\n",
    "    'adjclose': np.random.uniform(30, 50, 200).cumsum() + 100,\n",
    "    'volume': np.random.randint(10_000_000, 100_000_000, 200),\n",
    "    'ticker': ['GOOG'] * 200\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc837aaa",
   "metadata": {},
   "source": [
    "### Adjust high, low relative to open/close for realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data['high'] = np.maximum(dummy_data['open'], dummy_data['close']) + np.random.uniform(0.5, 2, 200)\n",
    "dummy_data['low'] = np.minimum(dummy_data['open'], dummy_data['close']) - np.random.uniform(0.5, 2, 200)\n",
    "\n",
    "df = pd.DataFrame(dummy_data, index=date_range)\n",
    "df.index.name = 'timestamp'\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9fd67",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ce0cd",
   "metadata": {},
   "source": [
    "### Add technical indicators like daily returns, moving averages, and volatility to enrich the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'close'\n",
    "\n",
    "# 1. Daily Returns\n",
    "df['daily_return'] = df[target_column].pct_change()\n",
    "\n",
    "# 2. Simple Moving Averages (SMA) - common lags 20, 50\n",
    "df['SMA_20'] = df[target_column].rolling(window=20).mean()\n",
    "df['SMA_50'] = df[target_column].rolling(window=50).mean()\n",
    "\n",
    "# 3. Exponential Moving Averages (EMA) - often more responsive\n",
    "df['EMA_20'] = df[target_column].ewm(span=20, adjust=False).mean()\n",
    "df['EMA_50'] = df[target_column].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "# 4. Volatility (Standard Deviation of returns over a period)\n",
    "df['volatility_20'] = df['daily_return'].rolling(window=20).std()\n",
    "\n",
    "# Remove rows with NaN values created by rolling windows\n",
    "df.dropna(inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba570f5d",
   "metadata": {},
   "source": [
    "## 4. Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282d17f",
   "metadata": {},
   "source": [
    "### Plot the close price, moving averages, volume, and daily returns to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5776855",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(df.index, df['close'], label='Close Price', color='blue')\n",
    "plt.plot(df.index, df['SMA_20'], label='SMA 20', color='green', linestyle='--')\n",
    "plt.plot(df.index, df['SMA_50'], label='SMA 50', color='red', linestyle='--')\n",
    "plt.title(f'Historical {df[\"ticker\"].iloc[0]} Close Price with Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(df.index, df['volume'], label='Volume', color='purple')\n",
    "plt.title(f'{df[\"ticker\"].iloc[0]} Daily Trading Volume')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(df.index, df['daily_return'], label='Daily Return', color='orange')\n",
    "plt.title(f'{df[\"ticker\"].iloc[0]} Daily Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Return')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc20ff0",
   "metadata": {},
   "source": [
    "## 5. Scale Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728ea2a",
   "metadata": {},
   "source": [
    "### Scale numerical features to the range [0, 1] using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = [col for col in df.columns if col not in ['ticker']]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[features_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=features_to_scale, index=df.index)\n",
    "\n",
    "print(\"\\nScaled DataFrame head (only showing first 5 rows and selected columns for brevity):\")\n",
    "print(scaled_df.head())\n",
    "print(\"\\nDescriptive statistics of scaled data:\")\n",
    "print(scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed5e5d",
   "metadata": {},
   "source": [
    "# 6. Create Sequences for LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa9ba0",
   "metadata": {},
   "source": [
    "### Create sequences of past `timesteps` days to predict the next day's close price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 60\n",
    "X = []\n",
    "y = []\n",
    "target_feature_index = scaled_df.columns.get_loc(target_column)\n",
    "\n",
    "for i in range(timesteps, len(scaled_df)):\n",
    "    X.append(scaled_df.iloc[i-timesteps:i].values)\n",
    "    y.append(scaled_df.iloc[i, target_feature_index])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"\\nShape of X (sequences): {X.shape}\")\n",
    "print(f\"Shape of y (target values): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d536f7",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a04606",
   "metadata": {},
   "source": [
    "### Split the sequences into training and testing sets chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_ratio = 0.8\n",
    "train_size = int(len(X) * train_split_ratio)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n--- Preprocessing and Data Preparation Complete ---\")\n",
    "print(\"You now have X_train, y_train, X_test, y_test ready for LSTM model training.\")\n",
    "print(\"The 'scaler' object is also preserved for inverse transforming predictions later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827f95e",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adf71c",
   "metadata": {},
   "source": [
    "### Save the processed data and scaler for use in the LSTM training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(features_to_scale, 'features_to_scale.pkl')\n",
    "print(\"\\nSaved X_train, X_test, y_train, y_test, scaler, and features_to_scale.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
